{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"snu_ppsml_note_10_ml_exercise.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"colab_type":"text","id":"gfrKNROhioFg"},"cell_type":"markdown","source":["## SNU PPSML - Machine Learning Exercise 3 (Note10)"]},{"metadata":{"colab_type":"text","id":"fIeqMK47xj1X"},"cell_type":"markdown","source":["#### **Contents**\n","---\n","* ML exercise 1: Gradient Descent [Optimization @ ML] (HW05)  "]},{"metadata":{"colab_type":"text","id":"n0PmZbFvxj_G"},"cell_type":"markdown","source":["---\n","* ML exercise 2: Neural Network [Representation @ ML] (실습과제 11/5-8 & 11/9 강의, **NNfactory.py**) \n","    - 입력 속성데이터와 지도라벨값 $\\{(x_i, y_i)\\}$ @ 인코딩: 입력과 출력층의 설정  \n","\n","    - 순전파 신경망(def feedforward())의 구성을 위한 여러가지 파라메터들     \n","        - $N_{layers}$ & $N_{nodes}$ in each layer\n","        - 가중치와 편향치 ($W$ & $B$)\n","        - 가중합(퍼셉트론 입력) : \n","        \n","        > $a_j=W_{ji} f_i(a_i)$ (전층의 출력 $f_i$에 대한 가중합)\n","        \n","        - 활성화 함수(출력) : $f_j(a_j)$\n","        \n","        > $f(a)=\\frac{1}{1+\\exp^{-a}}$ for sigmoid  \n","        \n","        > $f(a)=\\tanh(a)$ for tanh  \n","        \n","        > $f(a_k)=\\frac{\\exp[a_k]}{\\sum_{k'}\\exp[a_{k'}]}$ for softmax  \n","        \n","        > $f(a)=a$ for $ a > 0$ otherwise $0$ (ReLU)\n","        \n","        - 데이터의 순전파를 통한 최종 출력값 얻기: def feedforward(input_features)\n","        - ...\n","        \n","    - 오차보정의 역전파:  \n","    \n","        - 지도라벨값의 인코딩과 오차함수($E(w;x)$)의 정의\n","        - $\\delta_j$ (가중합 $a_j$에 대한 오차보정항 $\\equiv\\frac{\\partial E}{\\partial a_j}$)\n","        - $\\delta_j$를 통한 가중치보정 역전파의 구현: def backpropagate(target_label)\n","        - ...\n","        \n","    - 많은 데이터에 대한 학습 알고리즘 구현\n","    - check a contour of MLP's probability output for classification of 2D data  \n","    \n","---   \n","* **ML exercise 3: Training a NN for Regression** & Classification [**Evaluation**, Rep, Opt @ ML] (HW06-07, note09-10)\n","    - batch GD, mini-batch GD, stochastic GD [HW07]  \n","        - Visualize the minimizers in 2D\n","    - Validation of model, Over-fitting, Bias & Variance**  \n","        - Visualize an over-fitted status [note09]\n","    - Evaluation of model performance\n","        - error(loss), accuracy (...) --> learning curve [HW06]\n","        - NN score & ROC(Receiver Operating Characteristic) curve [note09, HW06]\n","    - **Training a NN for Regression [note10]**"]},{"metadata":{"colab_type":"code","id":"m7Ghj7f0ioFm","colab":{}},"cell_type":"code","source":["# Printing all outputs in a cell (not only the last output)\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"0DBgpcWDioHC"},"cell_type":"markdown","source":["----\n","### ** 3. Evaluation @ ML ** \n","\n","### For Supervised Learning (note10)\n","\n","1) batch gradient descent, mini-batch GD, stochastic GD (HW07)\n","\n","2) Validation of model\n","    - [과적합 (Over-fitting)상태의 확인] (note09)\n","\n","3) Evaluation of model performance      \n","    - [error/loss function] & [accuracy] (HW06)\n","    -  implement new error functions in your NN class  \n","       ('binary cross-entropy' & 'categorical cross-entropy') (HW07)\n","\n","    - [neural network score] (note09)\n","    - [ROC (Receiver Operating Characteristic) curve] & [its AUC (area under curve)] (note09, HW06)\n","     \n","**4) Regression **  \n","   1. training of a regression model\n","   2. validation of the regression model (과적합 상태의 생성 및 확인)"]},{"metadata":{"colab_type":"text","id":"Io4LHri1y66B"},"cell_type":"markdown","source":["---\n","#### ** 4) 회귀 (Regressions)** \n","\n","**1. Training of a regression model with 1D data $x$ with real number label $y$, $x\\rightarrow y(x)$**\n","\n","   - 주어진 1차원의 속성공간에 정의된 데이터 $x_i$들이 어떤 (연속/불연속) 실수값 $y_i$에 대응되고 있다고 하자.\n","       > $x_i \\, \\rightarrow \\, y_i\\, in\\, {\\mathbb R}$\n","       \n","     이때 $x_i \\, \\rightarrow \\, f(x_i;W_{trained}) = y_i$가 되도록, (신경망의/기계의) 출력모형 $f$를 데이터로부터 학습해내는 문제를 회귀(Regression)문제라 하며, 이는 분류문제(classification, $y_i$가 한정된 수량의 불연속값인 경우)와 더불어 머신러닝 지도학습(supervised learning)이 주요한 한 갈래가 된다. \n","\n","   - 회귀모형에서는 출력층의 활성화함수를 항등원 $f(a)=a$를 사용한다. NNfactory.MLP클래스를 활용하여 다음의 데이터를 학습해내는 신경망 회귀모형을 건설해보자.\n","\n","   - 훈련한 모형의 예측치를 matplotlib를 활용하여 시각화해보자.\n"]},{"metadata":{"colab_type":"code","id":"iLH90Tq-xVJj","outputId":"03e47ac5-7401-4ffc-9093-57527e0338b2","colab":{}},"cell_type":"code","source":["import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# 데이터 만들기 (속성 X)\n","X = np.linspace(0,1,51)\n","\n","# 데이터 만들기 (라벨 Y(for X) in real number)\n","a, b = 1., 0.001\n","Y_train = a*np.sin(2*np.pi*X) + b*np.random.randn(len(X))\n","Y_test = a*np.sin(2*np.pi*X) + b*np.random.randn(len(X))\n","\n","# reshape of X & Y for making main data block\n","X=X.reshape((len(X),1))\n","Y_train=Y_train.reshape((len(X),1))\n","Y_test=Y_test.reshape(len(X),1)\n","\n","# stacking two columns - Y, X, in to data_train/data_test\n","data_train = np.c_[Y_train,X]\n","data_test = np.c_[Y_test,X]\n","\n","# data check [y(x_i), x_i] \n","data_train[:10]\n","# data[0]\n","data_test[:10]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ -2.60706411e-04,   0.00000000e+00],\n","       [  1.26486057e-01,   2.00000000e-02],\n","       [  2.49667007e-01,   4.00000000e-02],\n","       [  3.68825971e-01,   6.00000000e-02],\n","       [  4.81271148e-01,   8.00000000e-02],\n","       [  5.87226664e-01,   1.00000000e-01],\n","       [  6.84140222e-01,   1.20000000e-01],\n","       [  7.70862658e-01,   1.40000000e-01],\n","       [  8.44046913e-01,   1.60000000e-01],\n","       [  9.04478607e-01,   1.80000000e-01]])"]},"metadata":{"tags":[]},"execution_count":39}]},{"metadata":{"id":"LayyH2tfLFqr","colab_type":"code","colab":{}},"cell_type":"code","source":["# 데이터 시각화\n","\n","fig1 = plt.figure(figsize=(8,4))\n","ax1 = fig1.add_subplot(111)\n","\n","ax1.plot(data_train[:,1],data_train[:,0], 'r.')\n","ax1.plot(data_test[:,1],data_test[:,0], 'b.')\n","ax1.set_xlabel('x', fontsize=20)\n","ax1.set_ylabel('$y(x)$', fontsize=20)\n","ax1.grid(True)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"bwLWLfpHxVIS","colab":{}},"cell_type":"code","source":["# 신경망 클래스 로딩\n","%reload_ext autoreload\n","%autoreload 2\n","\n","import NNfactory"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"V2p-ZQk9xVIW","colab":{}},"cell_type":"code","source":["model_str = '1:identity|'+1*'3:tanh|'+'1:identity'\n","lr = 0.001\n","name_tag = 'regression_lr'+str(lr)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"Af1nypdnxVIe","outputId":"bac85bc1-eeae-4e8b-fe4f-1e3040adc3cc","colab":{}},"cell_type":"code","source":["mynn = NNfactory.MLP(model_structure=model_str, \\\n","                     model_nametag=name_tag, \\\n","                     learning_rate=lr, \\\n","                     encoding='float')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n"," * 다음과 같은 구조의 다층퍼셉트론 연결이 초기화 되었습니다 *\n","\n"," > 모델이름 = regression_lr0.001\n"," > 총 층수 (입력 + 은닉(s) + 출력) =  3\n"," > 각 층에서의 노드수 =  [1, 3, 1]\n"," > 각 층에서의 활성화 함수 =  ['identity', 'tanh', 'identity']\n"," > 학습률(Learning Rate) =  0.001\n"," > 지도라벨 인코딩 방식 =  float\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","id":"lUlKiyYTxVIm","colab":{}},"cell_type":"code","source":["# 훈련 준비\n","data_type=None #mnist' # None\n","encoding='float' #'one-hot' #'integer' # 'float'\n","\n","# 훈련 & 테스트 데이터 준비\n","n_data_max=100  # 훈련에 사용할 데이터 갯수 (max = 10000)\n","n_data_test=1000  # 테스트에 사용할 테이터 갯수 (10000 - n_data_max)\n","\n","## 훈련 데이터\n","training_data_list = data_train[:n_data_max] # \n","\n","## 테스트 데이터\n","test_data_list = data_test[-n_data_test:]\n","\n","# 최대학습주기 설정\n","epochs = 50000"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"fayhtaSKxVI0","colab":{}},"cell_type":"code","source":["# 설정된 최대학습주기동안 훈련\n","\n","for e in range(epochs):\n","    \n","    id_data = 0\n","    \n","    for data in training_data_list[:n_data_max]:\n","        \n","        # 프로세스 게이지\n","        id_data += 1\n","        \n","        # 입력/지도 데이터 가공 \n","        if data_type == 'mnist':\n","            # split the mnist data by the ',' commas\n","            all_list = data.split(',')\n","            # 입력 속성 데이터 스케일링 (preprocessing)\n","            input_list = np.asfarray(all_list[1:])/255.0 #* 0.99) + 0.01\n","            # 지도 라벨 벡터 가공 (shape = (10,))\n","            target_list = np.zeros(10) #mynn.n_nodes[-1])\n","            # all_values[0] is the target label for this data\n","            target_list[int(all_list[0])] = 1.0\n","            \n","        else:\n","        \n","            input_list = data[1:] #np.asfarray(all_values[1:])        \n","            target_origin = data[0]\n","            \n","            if encoding == 'one-hot':\n","\n","                target_list = np.zeros(mynn.n_nodes[-1])\n","                target_list[int(data[0])] = 1\n","    \n","            elif encoding == 'integer':\n","                \n","                target_list = np.zeros(1)\n","                target_list[0] = int(data[0])\n","\n","            elif encoding == 'float':\n","                \n","                target_list = np.zeros(1)\n","                target_list[0] = data[0]\n","                \n","            else:\n","                raise ValueError(' => check your encoding scheme. ')\n","                \n","        mynn.train(input_list, target_list)\n","        \n","        pass\n","\n","    if (e%1000==0):\n","        print (' --------------------------------------')\n","        print(' * epoch = {}'.format(e+1))\n","        print(' > 훈련 샘플에 대한 성능 (정확도 & 평균에러) ')\n","        mynn.check_accuracy_error(training_data_list, 0, n_data_max-1, data_type=None)\n","        print('')\n","        print(' > 테스트 샘플에 대한 성능 (정확도 & 평균에러) ')\n","        mynn.check_accuracy_error(test_data_list, 0, n_data_test-1, data_type=None)\n","    \n","    \n","    pass"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"wXVquXY4xVI9"},"cell_type":"markdown","source":["---\n","* 훈련한 모형의 저장\n","\n","훈련된 모형은 NNfactory클래스 안의 save_model메소드를 사용하여 .npy포맷의 numpy array로 저장할 수 있다."]},{"metadata":{"colab_type":"code","id":"wkpqVG_ZxVI_","colab":{}},"cell_type":"code","source":["mynn.save_model(fname='mlp_reg_1tanh_epoch200k.npy', nametag='reg_1tanh')\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"sASjw8pxxVJD"},"cell_type":"markdown","source":["---\n","* 저장된 모형 불러오기 :\n","\n","저장된 .npy파일로부터 신경망정보가 담긴 넘파이 배열을 직접 로드하고, 이 넘파이 배열을 새 신경망 인스턴스 생성에 사용하여 저장된 모형과 똑같은 신경망을 로드한다."]},{"metadata":{"colab_type":"code","id":"q6iONXCexVJF","outputId":"56e496b9-8cbf-42c9-90d9-3f367b2d91a6","colab":{}},"cell_type":"code","source":["mynn_npy = np.load('mlp_reg_1tanh_epoch200k.npy')\n","mynn_load = NNfactory.MLP(load_model_np=mynn_npy)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n"," * 다음과 같은 구조의 다층퍼셉트론 모형이 \"load_model_np\" 정보로부터 로드되었습니다. *\n","\n"," > 모델이름 = reg_1tanh\n"," > 총 층수 (입력 + 은닉(s) + 출력) =  3\n"," > 각 층에서의 노드수 =  [1, 3, 1]\n"," > 각 층에서의 활성화 함수 =  ['identity', 'tanh', 'identity']\n"," > 학습률(Learning Rate) =  0.001\n"," > 지도라벨 인코딩 방식 =  float\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"0TGq4mAkxVJO"},"cell_type":"markdown","source":["---\n","* 학습한 회귀 모형을 시각화해보자"]},{"metadata":{"colab_type":"code","id":"elJ9ox_0xVJP","outputId":"08b7b6fe-287e-43b0-9ea8-57eebb2350f9","colab":{}},"cell_type":"code","source":["# data_train에 대한 회귀모형의 출력값을 얻고 저장.\n","result=[]\n","for data in data_train:\n","    result.append(mynn_load.feedforward(data[1:])[0])\n","   "],"execution_count":0,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'>\n","(51, 2)\n"],"name":"stdout"}]},{"metadata":{"id":"j0CS0lf3L7Qm","colab_type":"text"},"cell_type":"markdown","source":["---\n","* 잠깐!. 심심하니 학습한 회귀 모형을 분해해보자 "]},{"metadata":{"id":"Fkbr7iSCIRfq","colab_type":"code","outputId":"5ee0db0e-004a-4eeb-dee2-705274b98cdc","colab":{}},"cell_type":"code","source":["# 신경망 각 노드의 입력값\n","mynn_load.a\n","# 신경망 각 노드의 출력값\n","mynn_load.f\n","# 신경망 연결 가중치값\n","mynn_load.W\n","# 신경망 편향값\n","mynn_load.B\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([[ 1.]]), array([[-0.09474994],\n","        [-2.05878377],\n","        [-1.53500404]]), array([[ 0.00640182]])]"]},"metadata":{"tags":[]},"execution_count":82}]},{"metadata":{"id":"oHs1NbNVMesv","colab_type":"code","colab":{}},"cell_type":"code","source":["# 최종 은닉층 3개 (1|3|1)의 출력값과 가중치를 가지고 최종 출력노드의 출력값을 재구성\n","f=mynn_load.f[1].reshape(3)\n","w=mynn_load.W[1].reshape(3)\n","b=mynn_load.B[1]\n","\n","# 재구성한 출력값 1\n","y = np.dot(f,w) + b\n","print(y)\n","\n","# 현재 인스턴스가 가진 출력값\n","print(mynn_load.f[2])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IyDGTURQIRfx","colab_type":"code","colab":{}},"cell_type":"code","source":["# 심심하면.. \n","# 위의 예제에서\n","# 모든 데이터에 대한 출력을 다음의 4파트(가중치가 곱해진 은닉층의 각 노드출력 & 편향값, 각각의 출력모형)로 구분하여, \n","# 각각의 출력값을 역시 그래프(f' vs X)로 그려보고, \n","# 어떤 함수들이 어떤 가중치로 곱해져서,저런 훌륭한 회귀모형이 되었는지 감상해보자. \n","# y[x] = f1.W1[x] + f2.W2[x] + f3.w3[x] + b[x]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Wz-PTIUPItUy","colab_type":"text"},"cell_type":"markdown","source":["#### ** 4) [회귀(Regressions)]** \n","\n","**2. Validation of a Regression Model**\n","\n","   - 1의 회귀모형 훈련시에 데이터의 양을 줄이고 (&노이즈 추가), 동시에 신경망 모형의 표현능력을 키워서, 과적합 상태(overfitting)를 만들어보고 이를 시각화해보자.\n","\n"]},{"metadata":{"id":"mRu0JgVTIvga","colab_type":"code","colab":{}},"cell_type":"code","source":["# 데이터 만들기\n","\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Daa (X) generation\n","# X_train = np.linspace(0,1,10)\n","X_train = np.random.uniform(0,1,20)\n","X_train.sort()\n","X_test = np.random.uniform(0,1,10)\n","X_test.sort()\n","\n","# Label (=Y(X)) generation\n","a, b = 1., 0.2\n","Y_train = a*np.sin(2*np.pi*X_train) + b*np.random.randn(len(X_train))\n","Y_test = a*np.sin(2*np.pi*X_test) + b*np.random.randn(len(X_test))\n","\n","# reshape of X & Y for making main data block\n","X_train=X_train.reshape((len(X_train),1))\n","X_test=X_test.reshape((len(X_test),1))\n","\n","Y_train=Y_train.reshape((len(X_train),1))\n","Y_test=Y_test.reshape(len(X_test),1)\n","\n","# stacking two columns - Y and X, into data_train/test array \n","data_train = np.c_[Y_train,X_train]\n","data_test = np.c_[Y_test,X_test]\n","\n","# train data check [y(x_i), x_i] \n","data_train[:10]\n","# test data check \n","# data_test[:10]\n","\n","# # data save\n","# np.save('data_reg_overfit_train.npy',data_train)\n","# np.save('data_reg_overfit_test.npy',data_test)\n","\n","# # data load from file\n","# data_train = np.load('data_reg_overfit_train.npy')\n","# data_test= np.load('data_reg_overfit_test.npy')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ryqtMdsXJCBs","colab_type":"code","colab":{}},"cell_type":"code","source":["## 데이터 시각화\n","fig1 = plt.figure(figsize=(8,4))\n","ax1 = fig1.add_subplot(111)\n","\n","ax1.plot(data_train[:,1],data_train[:,0], 'r.')\n","ax1.plot(data_test[:,1],data_test[:,0], 'b.')\n","ax1.set_xlabel('x', fontsize=20)\n","ax1.set_ylabel('$y(x)$', fontsize=20)\n","ax1.grid(True)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DL-H3trbJPIg","colab_type":"code","colab":{}},"cell_type":"code","source":["%reload_ext autoreload\n","%autoreload 2\n","\n","import NNfactory"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_CHQS0jgJSWD","colab_type":"code","colab":{}},"cell_type":"code","source":["model_str = '1:identity|'+6*'100:relu|'+'1:identity'\n","lr = 0.001\n","name_tag = 'regression_lr'+str(lr)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IVW9tdw8JTEj","colab_type":"code","colab":{}},"cell_type":"code","source":["mynn = NNfactory.MLP(model_structure=model_str, \\\n","                     model_nametag=name_tag, \\\n","                     learning_rate=lr, \\\n","                     encoding='float')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HUiFCQnJJTMh","colab_type":"code","colab":{}},"cell_type":"code","source":["# 훈련 준비\n","data_type=None #mnist' # None\n","encoding='float' #'one-hot' #'integer' # 'float'\n","\n","# 훈련 & 테스트 데이터 준비\n","n_data_max=20  # 훈련에 사용할 데이터 갯수 (max = 10000)\n","n_data_test=20  # 테스트에 사용할 테이터 갯수 (10000 - n_data_max)\n","\n","## 훈련 데이터\n","training_data_list = data_train[:n_data_max] # \n","\n","## 테스트 데이터\n","test_data_list = data_test[-n_data_test:]\n","\n","# 최대학습주기 설정\n","epochs = 50000"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gjGvMazdJbEd","colab_type":"code","colab":{}},"cell_type":"code","source":["# 설정된 최대학습주기동안 훈련\n","\n","for e in range(epochs):\n","    \n","    id_data = 0\n","    \n","    for data in training_data_list[:n_data_max]:\n","        \n","        # 프로세스 게이지\n","        id_data += 1\n","        \n","        # 입력/지도 데이터 가공 \n","        if data_type == 'mnist':\n","            # split the mnist data by the ',' commas\n","            all_list = data.split(',')\n","            # 입력 속성 데이터 스케일링 (preprocessing)\n","            input_list = np.asfarray(all_list[1:])/255.0 #* 0.99) + 0.01\n","            # 지도 라벨 벡터 가공 (shape = (10,))\n","            target_list = np.zeros(10) #mynn.n_nodes[-1])\n","            # all_values[0] is the target label for this data\n","            target_list[int(all_list[0])] = 1.0\n","            \n","        else:\n","        \n","            input_list = data[1:] #np.asfarray(all_values[1:])        \n","            target_origin = data[0]\n","            \n","            if encoding == 'one-hot':\n","\n","                target_list = np.zeros(mynn.n_nodes[-1])\n","                target_list[int(data[0])] = 1\n","    \n","            elif encoding == 'integer':\n","                \n","                target_list = np.zeros(1)\n","                target_list[0] = int(data[0])\n","\n","            elif encoding == 'float':\n","                \n","                target_list = np.zeros(1)\n","                target_list[0] = data[0]\n","                \n","            else:\n","                raise ValueError(' => check your encoding scheme. ')\n","                \n","        mynn.train(input_list, target_list)\n","        \n","        pass\n","\n","    if (e%1000==0):\n","        print (' --------------------------------------')\n","        print(' * epoch = {}'.format(e+1))\n","        print(' > 훈련 샘플에 대한 성능 (정확도 & 평균에러) ')\n","        mynn.check_accuracy_error(training_data_list, 0, n_data_max-1, data_type=None)\n","        print('')\n","        print(' > 테스트 샘플에 대한 성능 (정확도 & 평균에러) ')\n","        mynn.check_accuracy_error(test_data_list, 0, n_data_test-1, data_type=None)\n","    \n","    \n","    pass"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BUYcVpbhJfPR","colab_type":"text"},"cell_type":"markdown","source":["---\n","* 훈련한 모형의 저장\n","\n","훈련된 모형은 NNfactory클래스 안의 save_model메소드를 사용하여 .npy포맷의 numpy array로 저장할 수 있다."]},{"metadata":{"id":"yLqBEa9XJf8N","colab_type":"code","colab":{}},"cell_type":"code","source":["mynn.save_model(fname='mlp_reg_overfit.npy', nametag='reg_overfit')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cElq4yYYJmnT","colab_type":"text"},"cell_type":"markdown","source":["---\n","* 저장된 모형 불러오기 :\n","\n","저장된 .npy파일로부터 신경망정보가 담긴 넘파이 배열을 직접 로드하고, 이 넘파이 배열을 새 신경망 인스턴스 생성에 사용하여 저장된 모형과 똑같은 신경망을 로드한다."]},{"metadata":{"id":"6GQ2KXg-JibH","colab_type":"code","colab":{}},"cell_type":"code","source":["mynn_npy = np.load('mlp_reg_overfit.npy')\n","mynn_load = NNfactory.MLP(load_model_np=mynn_npy)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XcuKvu-pJute","colab_type":"text"},"cell_type":"markdown","source":["---\n","* 과적합 상태의 회귀 모형을 시각화해보기"]},{"metadata":{"id":"yC23ciyhJii-","colab_type":"code","colab":{}},"cell_type":"code","source":["# 과적합 상태의 모형 출력 확인을 위한 잘 정렬된 테스트 데이터 (X_val) 정의하기\n","X_val = np.linspace(0,1,1000)\n","X_val = X_val.reshape((len(X_val),1))\n","\n","# X_val 데이터에 대한 회귀모형 출력값 얻기\n","result=[]\n","for X in X_val:\n","    result.append(mynn_load.feedforward(X)[0])   "],"execution_count":0,"outputs":[]},{"metadata":{"id":"Rc_ZV04zJisS","colab_type":"code","colab":{}},"cell_type":"code","source":["## plot objects \n","fig1 = plt.figure(figsize=(8,4))\n","ax1 = fig1.add_subplot(111)\n","\n","# 과적합 상태의 모형의 훈련에 쓰인 훈련데이터 플랏\n","ax1.plot(data_train[:,1],data_train[:,0], 'g*',label='train data')\n","\n","# 과적합 모형의 출력을 X_val를 사용하여 플랏\n","ax1.plot(X_val,result, 'b-',label='regression output')\n","\n","ax1.set_xlabel('x', fontsize=20)\n","ax1.set_ylabel('$y(x)$', fontsize=20)\n","ax1.legend(loc='best')\n","ax1.grid(True)\n","\n","plt.show()"],"execution_count":0,"outputs":[]}]}